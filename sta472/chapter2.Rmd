
---
title: "Chapter 2: Bayes' Rule - Simplified"
author: "Simplified by Dr. Rajitha M. Silva"
output: html_document
---

# Introduction

In this lecture, we focus on **Bayes' Rule**, the heart of Bayesian statistics. We'll use real-world examples-like detecting fake news based on punctuation-to understand how we update beliefs based on new evidence.

This chapter introduces essential tools for Bayesian thinking: **marginal, conditional, joint probabilities**, and culminates in **Bayes' Rule**. All examples and calculations use both R and Python.

---

# The Fake News Example

We're examining whether a news article is **fake** or **real** based on whether its **title contains an exclamation mark**.

Based on the dataset:

<div style="text-align:center">

| title_has_excl | Fake | Real |
|:---------------|-----:|-----:|
| FALSE          |   44 |   88 |
| TRUE           |   16 |    2 |
| **Total**      |   60 |   90 |

</div>

From this we know:
- 16 fake and 2 real articles contain an exclamation mark.
- Total number of articles is 150.

---

# Step 1: Conditional Probability

We first calculate two important conditional probabilities:

- \( P(! \mid Fake) = \frac{16}{60} \)
- \( P(Fake \mid !) = \frac{16}{18} \)

These help us understand how often fake articles include exclamations and how likely an article is fake, **given** it contains one.

```{r}
p_exclamation_given_fake <- 16 / 60
p_fake_given_exclaim <- 16 / 18

p_exclamation_given_fake
p_fake_given_exclaim
```

```{python}
p_exclamation_given_fake = 16 / 60
p_fake_given_exclaim = 16 / 18

print("P(! | Fake):", round(p_exclamation_given_fake, 3))
print("P(Fake | !):", round(p_fake_given_exclaim, 3))
```

---

# Step 2: Bayes' Rule Setup

Bayes' Rule lets us reverse conditional probabilities:

\[
P(Fake \mid !) = \frac{P(! \mid Fake) \cdot P(Fake)}{P(!)}
\]

With:

- \( P(! \mid Fake) = 16/60 \)
- \( P(Fake) = 60/150 \)
- \( P(!) = (16+2)/150 = 18/150 \)

```{r}
likelihood <- 16 / 60
prior <- 60 / 150
evidence <- 18 / 150

posterior <- (likelihood * prior) / evidence
posterior
```

```{python}
likelihood = 16 / 60
prior = 60 / 150
evidence = 18 / 150

posterior = (likelihood * prior) / evidence
print("Posterior P(Fake | !) using Bayes' Rule:", round(posterior, 3))
```

We get a posterior probability of ??? 0.889 - meaning that if we see an article with an exclamation mark, it's very likely to be fake.

---

# Conditional vs Reverse Conditional

Let's highlight again:

- \( P(! \mid Fake) = \frac{16}{60} = 0.267 \)
- \( P(Fake \mid !) = \frac{16}{18} \approx 0.889 \)

This shows how different the probabilities can be when the condition is flipped.

```{r}
p1 <- 16 / 60
p2 <- 16 / 18

c(p1 = p1, p2 = p2)
```

```{python}
print({"P(! | Fake)": 16/60, "P(Fake | !)": 16/18})
```

---

# Summary of Terms

<div style="text-align:center">

| Term         | Meaning                                  |
|--------------|-------------------------------------------|
| Prior        | Our initial belief (e.g., P(Fake) = 60/150) |
| Likelihood   | P(! | Fake): Chance of seeing data if hypothesis is true |
| Evidence     | P(!): Overall chance of seeing the data   |
| Posterior    | Updated belief after seeing the data      |

</div>

---

# Key Takeaway

- \( P(A | B) 
\neq P(B | A) \)
- Bayes' Rule is the bridge between these - from **data to belief**.


# Independence of Events

Sometimes, learning new information **does not** change our belief about another event. In such cases, we say the events are **independent**.

## Example: Shoes and Feet

Let's say your friend owns **two pairs of shoes**:

- One **yellow pair**
- One **blue pair**

Total: **4 shoes** - yellow-left, yellow-right, blue-left, and blue-right.

Suppose they pick one shoe at random and don't show it to you. Initially:

$$
P(\text{Right Foot}) = \frac{2}{4} = 0.5
$$

Now they tell you:
"I picked one of the **yellow shoes**."

There are two yellow shoes - one left, one right - and they still picked one at random. So:

$$
P(\text{Right Foot} \mid \text{Yellow}) = \frac{1}{2} = 0.5
$$

That is, **learning the shoe's color didn't change our belief about which foot it's for**.

> Shoe color and foot are **independent**.

---

## Definition: Independent Events

Two events **A** and **B** are independent if:

$$
P(A \mid B) = P(A)
$$

Equivalently, if \(P(B) > 0\):

$$
P(A \cap B) = P(A) \cdot P(B)
$$

---

##  Key Insight

- If learning that B occurred **does change** the probability of A, A and B are **dependent**.
- If it **does not change** the probability of A, A and B are **independent**.

In the shoe example:
- Let A = "Right Foot"
- Let B = "Yellow Shoe"

Then:

$$
P(A \mid B) = P(A) = 0.5 \Rightarrow A \text{ and } B \text{ are independent}
$$
# Probability vs. Likelihood

Let's reexamine our fake news example with these conditional concepts in place. Earlier, we calculated:

- \( P(! \mid 	\text{Fake}) = 16 / 60 = 0.267 \)
- \( P(	\text{Fake} \mid !) = 16 / 18 \approx 0.889 \)

At first glance, these might seem like they are answering the same question - but they're **not**. They differ in direction and purpose.

---

## Probability: Forward Thinking

**Probability** answers:

> "If the world behaves a certain way, what data would I expect to see?"

In our example:

- \( P(! \mid 	\text{Fake}) \) is the probability of seeing an exclamation mark, assuming the article is fake.
- This is useful when simulating data: if we know something is fake, what's the chance it shouts with a "!"?

---

## Likelihood: Reverse Thinking

**Likelihood** flips the problem and asks:

> "Given that I saw some data, how plausible is each possible world (or parameter value)?"

In our example:

- We observe the data: the article has an exclamation mark (!)
- We then compare:
  - How likely is this if the article is fake? ??? \( P(! \mid 	\text{Fake}) \)
  - How likely is this if the article is real? ??? \( P(! \mid 	\text{Real}) = 2 / 90 
\approx 0.022 \)

> We use these to judge **which hypothesis is more supported by the data**.

---

## Connection: Likelihood feeds Bayes' Rule

Bayes' Rule takes:

- **Prior** belief about Fake/Real
- **Likelihood** of seeing an exclamation mark under each scenario
- Combines them to yield a **posterior** probability:
  
\[
P(	\text{Fake} \mid !) = \frac{P(! \mid 	\text{Fake}) \cdot P(	\text{Fake})}{P(!)}
\]

Here, the likelihood values \(P(! \mid 	\text{Fake})\) and \(P(! \mid 	\text{Real})\) help adjust our belief.

---

##  Key Insight

- **Probability**: "Given the world, what's the chance of the data?"
- **Likelihood**: "Given the data, how plausible is the world?"

They use the same math (e.g., conditional probability), but for different purposes.


---

# Probability vs. Likelihood 

Let's return to our fake news scenario and deepen our understanding of **probability** and **likelihood**-two ideas that look similar but serve different purposes.

We previously computed:

- \( P(! \mid 	\text{Fake}) = 16 / 60 = 0.267 \)
- \( P(	\text{Fake} \mid !) = 16 / 18 \approx 0.889 \)

These quantities might appear to be just inverses of each other-but they're actually answering different types of questions. Let's clarify.

---

##  What Is Probability?

**Probability** quantifies the chance of an event happening **given some condition** or a model of the world. In the context of our fake news dataset:

- \( P(! \mid 	\text{Fake}) \): The chance of an article **having an exclamation mark**, given that it is **fake**.

This is a **forward-thinking** question: if we know the type of article, what can we expect to see in terms of features?

It's useful in **generative settings** where we simulate or predict data from assumed conditions.

---

##  What Is Likelihood?

Now flip the direction. Suppose you observe some **data**-say, an exclamation mark-and ask:

> "How consistent is this data with various possible explanations of the world?"

This is **likelihood**.

For example:
- The **likelihood** of the data (exclamation mark) under the assumption that the article is fake:  
  \( L(	\text{Fake}) = P(! \mid 	\text{Fake}) = 16/60 \)

- The **likelihood** of the data under the real news model:  
  \( L(	\text{Real}) = P(! \mid 	\text{Real}) = 2/90 \approx 0.022 \)

So while both quantities **use conditional probability**, **likelihood is about comparing models**, not predicting outcomes.

---

##  Side-by-Side Comparison

<div style="text-align:center">

| Concept        | Perspective            | Example                             | Purpose                     |
|----------------|------------------------|-------------------------------------|-----------------------------|
| Probability    | Given the model, how likely is the data? | \( P(! \mid 	\text{Fake}) \) | Predict data under a model |
| Likelihood     | Given the data, how likely is the model? | \( L(\text{Fake}) = P(! \mid \text{Fake}) \) | Compare competing hypotheses |

</div>

---

##  In Bayesian Updating

Bayes' Rule uses **likelihood** to adjust **prior beliefs**:

\[
P(\text{Fake} \mid !) = \frac{P(! \mid 	\text{Fake}) \cdot P(\text{Fake})}{P(!)}
\]

Here's how it fits:

- The **prior** \(P(\text{Fake})\) reflects our belief before seeing the title.
- The **likelihood** \(P(! \mid 	\text{Fake})\) tells us how well this belief aligns with the observed data.
- The **evidence** \(P(!)\) ensures we normalize across all models.
- The **posterior** \(P(	\text{Fake} \mid !)\) is our updated belief.

---

##  Final Insight

> Probability answers:  
> *"What's the chance of this data under my model?"*

> Likelihood answers:  
> *"How much support does this data give each model?"*

These are not just semantics-they are **core ideas** in Bayesian inference, and essential for machine learning, hypothesis testing, and decision theory.




# Law of Total Probability and Bayes' Theorem

##  Law of Total Probability (LTP)

In Bayesian thinking, the Law of Total Probability helps us calculate the **marginal probability** of observed data by **summing over all possible causes**.

###  General Form

If \( A \) is an observed event, and \( B \), \( B^c \) are two possible states of the world, then:

$$
P(A) = P(A \cap B) + P(A \cap B^c) = P(A \mid B)P(B) + P(A \mid B^c)P(B^c)
$$

This is useful when we want to know how likely some data \( A \) is **regardless** of which hypothesis is true.

### Fake News Example

We observed that an article contains an exclamation mark (!). The marginal probability of this happening is:

$$
P(!) = P(! \mid \text{Fake}) \cdot P(\text{Fake}) + P(! \mid \text{Real}) \cdot P(\text{Real})
$$

Given:
- \( P(! \mid \text{Fake}) = 0.2667 \)
- \( P(! \mid \text{Real}) = 0.0222 \)
- \( P(\text{Fake}) = 0.4 \)
- \( P(\text{Real}) = 0.6 \)

Then:
$$
P(!) = 0.2667 \cdot 0.4 + 0.0222 \cdot 0.6 = 0.1067 + 0.0133 = 0.12
$$

So, 12% of all articles use an exclamation mark.

---

##  Bayes' Theorem

Bayes' Theorem allows us to **reverse the condition**. That is, compute:

$$
P(B \mid A) = \frac{P(B \cap A)}{P(A)} = \frac{P(B) \cdot P(A \mid B)}{P(A)}
$$

This formula is the backbone of Bayesian inference.

### Fake News Posterior

We use Bayes' Rule to update our belief that an article is **fake**, given we saw an exclamation mark:

$$
P(\text{Fake} \mid !) = \frac{P(! \mid \text{Fake}) \cdot P(\text{Fake})}{P(!)}
= \frac{0.2667 \cdot 0.4}{0.12} = 0.889
$$

###  Summary Table

<div style="text-align:center">

| Step       | Value         |
|------------|---------------|
| Prior      | \( P(\text{Fake}) = 0.4 \) |
| Likelihood | \( P(! \mid \text{Fake}) = 0.2667 \) |
| Evidence   | \( P(!) = 0.12 \) |
| Posterior  | \( P(\text{Fake} \mid !) = 0.889 \) |

</div>

> Bayes' Rule = Prior × Likelihood / Evidence

This tells us: **Despite only 40% of articles being fake**, seeing an exclamation mark (more common in fake news) strongly shifts our belief to 88.9%.


# Posterior Simulation (Section 2.1.5)

Once we've calculated a posterior probability using Bayes' Rule, we can use **simulation** to better understand it. This is especially useful when:

- Probabilities are hard to compute exactly
- We want to estimate outcomes in large samples

Let's walk through an example using our fake news data.

## Scenario

We already computed the posterior probability that a news article is fake **given** that it contains an exclamation mark:

$$
P(\text{Fake} \mid !) = 0.889
$$

Let's simulate this situation:

---

##  R Simulation: Sampling Based on Posterior

```{r}
set.seed(123)

# Simulate 10,000 articles with exclamations
posterior_samples <- sample(c("Fake", "Real"),
                            size = 10000,
                            replace = TRUE,
                            prob = c(0.889, 0.111))

# Estimate posterior from simulation
table(posterior_samples)
mean(posterior_samples == "Fake")
```

---

##  Python Simulation

```{python}
import numpy as np
import pandas as pd

np.random.seed(123)

# Simulate 10,000 news articles based on posterior probability
samples = np.random.choice(["Fake", "Real"],
                           size=10000,
                           p=[0.889, 0.111])

# Convert to pandas for easy analysis
samples_df = pd.DataFrame(samples, columns=["Label"])
posterior_mean = (samples_df["Label"] == "Fake").mean()

print("Proportion of 'Fake' articles in simulation:", posterior_mean)
```

---

##  Why Simulate?

Simulation helps you:
- Estimate **frequencies** when you can't easily compute them
- Visualize **uncertainty**
- Prepare for **Bayesian workflows** in more complex models (e.g., MCMC)

In this example, the result will be very close to the analytical posterior (0.889) - confirming our calculations.


---

# Visualizing Posterior Simulation (Figure 2.2)

After simulating from the posterior distribution, we can visualize the results. This bar plot shows how many of the 10,000 sampled articles were labeled "Fake" vs "Real".

##  R Code for Posterior Bar Plot

```{r fig.height=4, fig.width=6}
set.seed(123)

# Simulate 10,000 draws
posterior_samples <- sample(c("Fake", "Real"),
                            size = 10000,
                            replace = TRUE,
                            prob = c(0.889, 0.111))

# Create a table of counts
sample_table <- table(posterior_samples)

# Bar plot
barplot(sample_table,
        col = c("forestgreen", "steelblue"),
        main = "Simulated Article Status (Posterior Sample)",
        ylab = "Count",
        xlab = "Article Status")
```

---

This bar chart provides a visual confirmation of our posterior belief: most articles with exclamation marks are likely fake.

## Section 2.2: Example - Pop vs Soda vs Coke

Let's apply Bayes' Rule to another scenario involving regional terminology preferences for carbonated beverages in the United States. Suppose you hear someone refer to a soft drink as "pop." Without knowing anything else, what is the most likely U.S. region they are from?

We define four regions: Midwest (M), Northeast (N), South (S), and West (W). The prior probabilities of being from each region, based on U.S. Census data, are:

<div style="text-align:center">

| Region   | Probability |
|:--------:|:-----------:|
| Midwest  | 0.21        |
| Northeast| 0.17        |
| South    | 0.38        |
| West     | 0.24        |

</div>

### Observed Data: Use of the Word "Pop"

We now observe the use of the term "pop." From the `pop_vs_soda` dataset in the `bayesrules` R package, the estimated probabilities (likelihoods) of using the word "pop" in each region are derived as follows:

```{r}
# Load the data
library(bayesrules)
library(janitor)
library(tidyverse)
data(pop_vs_soda)

# Summarize pop use by region
pop_vs_soda %>%
  tabyl(pop, region) %>%
  adorn_percentages("col")
```

Extracting what we need from the output:

<div style="text-align:center">

| Region   | \( P(\text{pop} \mid \text{Region}) \) |
|:--------:|:--------------------------------------:|
| Midwest  | 0.6447                                |
| Northeast| 0.2734                                |
| South    | 0.0792                                |
| West     | 0.2943                                |

</div>

### Step 1: Calculate Marginal Probability of Saying "Pop"

Using the law of total probability:

\[
P(A) = \sum P(A \mid R_i)P(R_i) = 0.6447*0.21 + 0.2734*0.17 + 0.0792*0.38 + 0.2943*0.24 \approx 0.2826
\]

### Step 2: Calculate Posterior Probabilities Using Bayes' Rule

Bayes' Rule:

\[
P(R_i \mid A) = \frac{P(R_i) P(A \mid R_i)}{P(A)}
\]

#### R Code
```{r}
# Prior probabilities
prior <- c(M = 0.21, N = 0.17, S = 0.38, W = 0.24)

# Likelihood of saying "pop"
likelihood <- c(M = 0.6447, N = 0.2734, S = 0.0792, W = 0.2943)

# Marginal probability of "pop"
pop_marginal <- sum(prior * likelihood)

# Posterior probabilities
posterior <- (prior * likelihood) / pop_marginal
round(posterior, 4)
```

#### Python Code
```python
import numpy as np

prior = np.array([0.21, 0.17, 0.38, 0.24])
likelihood = np.array([0.6447, 0.2734, 0.0792, 0.2943])

# Marginal probability
pop_marginal = np.sum(prior * likelihood)

# Posterior
posterior = (prior * likelihood) / pop_marginal
np.round(posterior, 4)
```

Extracting what we need from the Output:

<div style="text-align:center">

| Region   | Posterior Probability |
|:--------:|:---------------------:|
| Midwest  | 0.4791                |
| Northeast| 0.1645                |
| South    | 0.1065                |
| West     | 0.2499                |

</div>

### Interpretation
Despite the South being the most populous region (with a prior of 0.38), the data (use of the word "pop") strongly points to the Midwest, boosting the posterior probability for the Midwest to 47.9%. This is a classic case where the likelihood influences the posterior significantly.

## Section 2.3: Building a Bayesian model for random variables

We now turn to modeling **numerical** outcomes using Bayes' Rule. Consider Kasparov vs. Deep Blue in a 6-game chess match. Our interest is in estimating Kasparov's win probability \( \pi \).

### 2.3.1 Prior Probability Model

Suppose we believe Kasparov's win probability \( \pi \) could be 0.2, 0.5, or 0.8 with prior probabilities 0.10, 0.25, and 0.65, respectively.

<div style="text-align:center">

| \(\pi\) | Prior \( f(\pi) \) |
|:------:|:-----------------:|
| 0.2    | 0.10              |
| 0.5    | 0.25              |
| 0.8    | 0.65              |

</div>

### 2.3.2 The Binomial Data Model

Let \( Y \) be the number of wins in 6 games. Then:

\[
Y | \pi \sim \text{Bin}(6, \pi)
\]

Example:

\[
P(Y = 6 | \pi = 0.8) = {6 \choose 6}(0.8)^6 = 0.2621
\]

### 2.3.3 The Binomial Likelihood Function

Suppose Kasparov wins 1 game. The likelihood function is:

\[
L(\pi | y = 1) = {6 \choose 1}\pi(1-\pi)^5 = 6\pi(1-\pi)^5
\]

Using R:
```{r}
# Likelihood values
pi_vals <- c(0.2, 0.5, 0.8)
likelihood <- 6 * pi_vals * (1 - pi_vals)^5
likelihood
```

Using Python:
```python
pi_vals = np.array([0.2, 0.5, 0.8])
likelihood = 6 * pi_vals * (1 - pi_vals)**5
likelihood
```

### 2.3.4 Normalizing Constant

Compute \( f(y=1) \):

\[
f(y=1) = \sum f(\pi)L(\pi | y = 1)
\]

#### R Code
```{r}
prior <- c(0.10, 0.25, 0.65)
f_y1 <- sum(prior * likelihood)
f_y1
```

#### Python Code
```python
prior = np.array([0.10, 0.25, 0.65])
f_y1 = np.sum(prior * likelihood)
f_y1
```

### 2.3.5 Posterior Probability Model

\[
f(\pi | y = 1) = \frac{f(\pi)L(\pi|y=1)}{f(y=1)}
\]

#### R Code
```{r}
posterior <- prior * likelihood / f_y1
round(posterior, 3)
```

#### Python Code
```python
posterior = prior * likelihood / f_y1
np.round(posterior, 3)
```

### 2.3.6 Posterior Shortcut

Use unnormalized posterior and normalize manually:

#### R Code
```{r}
unnorm_post <- prior * likelihood
posterior <- unnorm_post / sum(unnorm_post)
posterior
```

#### Python Code
```python
unnorm_post = prior * likelihood
posterior = unnorm_post / np.sum(unnorm_post)
posterior
```

### 2.3.7 Posterior Simulation

#### R Code
```{r}
chess <- data.frame(pi = c(0.2, 0.5, 0.8))
prior <- c(0.10, 0.25, 0.65)

set.seed(84735)
chess_sim <- sample_n(chess, size = 10000, weight = prior, replace = TRUE)
chess_sim <- chess_sim %>% mutate(y = rbinom(10000, size = 6, prob = pi))

win_one <- chess_sim %>% filter(y == 1)
win_one %>% tabyl(pi) %>% adorn_totals("row")
```

#### Python Code
```python
import pandas as pd
np.random.seed(84735)

pi_values = np.array([0.2, 0.5, 0.8])
prior = np.array([0.10, 0.25, 0.65])

sim_pi = np.random.choice(pi_values, size=10000, p=prior)
sim_y = np.random.binomial(n=6, p=sim_pi)

df = pd.DataFrame({'pi': sim_pi, 'y': sim_y})
posterior_sim = df[df['y'] == 1]['pi'].value_counts(normalize=True).sort_index()
posterior_sim
```

This simulation supports our theoretical results by approximating the posterior probabilities through repeated sampling.

---
