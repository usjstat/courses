---
title: "Answers to Exercises - Chapter 4"
author: "Dr. Rajitha M. Silva"
output: html_document
---

## Exercise 4.1: Match the Prior to the Description

### Question
Five different prior models for \(\pi\) are listed below. Label each one with one of these descriptors:

- Strongly favoring \(\pi < 0.5\)
- Somewhat favoring \(\pi < 0.5\)
- Centering \(\pi\) on 0.5
- Somewhat favoring \(\pi > 0.5\)
- Strongly favoring \(\pi > 0.5\)

**Prior Models:**

a) Beta(1.8, 1.8)  
b) Beta(3, 2)  
c) Beta(1, 10)  
d) Beta(1, 3)  
e) Beta(17, 2)  

### Answer
We analyze each Beta distribution based on its shape and center (mean = \( \frac{\alpha}{\alpha + \beta} \)):

- **Beta(1.8, 1.8)** → Mean = 0.5 → **Centering \(\pi\) on 0.5**
- **Beta(3, 2)** → Mean = 0.6 → **Somewhat favoring \(\pi > 0.5\)**
- **Beta(1, 10)** → Mean = 0.0909 → **Strongly favoring \(\pi < 0.5\)**
- **Beta(1, 3)** → Mean = 0.25 → **Somewhat favoring \(\pi < 0.5\)**
- **Beta(17, 2)** → Mean = 0.895 → **Strongly favoring \(\pi > 0.5\)**

### Final Labels

a) Beta(1.8, 1.8): Centering \(\pi\) on 0.5  
b) Beta(3, 2): Somewhat favoring \(\pi > 0.5\)  
c) Beta(1, 10): Strongly favoring \(\pi < 0.5\)  
d) Beta(1, 3): Somewhat favoring \(\pi < 0.5\)  
e) Beta(17, 2): Strongly favoring \(\pi > 0.5\)  

---

## Exercise 4.14: Challenge – Posterior Mode

### Question

In the Beta-Binomial setting, show that we can write the posterior mode of \(\pi\) as a weighted average of the prior mode and observed sample success rate:

\[
\text{Mode}(\pi\mid Y = y) = \frac{\alpha+\beta-2}{\alpha+\beta+n-2} \cdot \text{Mode}(\pi) +
\frac{n}{\alpha+\beta+n-2} \cdot \frac{y}{n}
\]

To what value does the posterior mode converge as our sample size \(n\) increases? Support your answer with evidence.

### Proof Sketch
We know that:
- The mode of a Beta(\(\alpha, \beta\)) distribution is \(\frac{\alpha - 1}{\alpha + \beta - 2}\) for \(\alpha, \beta > 1\).
- For the posterior Beta(\(\alpha + y, \beta + n - y\)), the mode is:
\[
\frac{\alpha + y - 1}{\alpha + \beta + n - 2}
\]

Rewriting:
\[
\frac{\alpha + y - 1}{\alpha + \beta + n - 2} =
\frac{\alpha + \beta - 2}{\alpha + \beta + n - 2} \cdot \frac{\alpha - 1}{\alpha + \beta - 2} +
\frac{n}{\alpha + \beta + n - 2} \cdot \frac{y}{n}
\]

This shows the posterior mode is a weighted average of the prior mode and observed success rate.

### Interpretation
As \(n \to \infty\), the term \(\frac{\alpha + \beta - 2}{\alpha + \beta + n - 2}\) approaches 0, and:
\[
\text{Mode}(\pi\mid Y = y) \to \frac{y}{n}
\]

Thus, the posterior mode converges to the sample proportion, as expected.

### R Code (Optional Simulation)
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(bayesrules)
library(ggplot2)
library(dplyr)  # <--- This line is critical

alpha <- 3
beta <- 2
prior_mode <- (alpha - 1) / (alpha + beta - 2)
n_vals <- seq(5, 1000, by = 5)
y_vals <- 0.6 * n_vals

post_modes <- (alpha + y_vals - 1) / (alpha + beta + n_vals - 2)

plot(n_vals, post_modes, type = "l", col = "blue", lwd = 2,
     xlab = "Sample Size (n)", ylab = "Posterior Mode",
     main = "Posterior Mode Converging to Sample Proportion")
abline(h = 0.6, col = "red", lty = 2)

```

The blue line shows posterior mode values for increasing \(n\), and the red dashed line indicates the observed success rate (\(0.6\)).

This visual confirms that the posterior mode approaches the observed success rate as more data is collected.

---

## Exercise 4.17: Different Data, Different Posteriors

### a) Sketch and Describe the Prior
```{r}
library(bayesrules)
plot_beta(4, 3)
```

The prior \( \pi \sim \text{Beta}(4, 3) \) reflects a moderate belief that a user is more likely than not to click the ad. The distribution peaks around \( \pi = 0.6 \), suggesting a central tendency favoring ad engagement.

### b) Specify Posterior Models from Scratch

- **Employee 1**: 0 clicks out of 1 trial.  
  Prior: \( f(\pi) \propto \pi^3 (1 - \pi)^2 \)  
  Likelihood: \( L(\pi) = (1 - \pi)^1 \)  
  Posterior: \( f(\pi | y) \propto \pi^3 (1 - \pi)^3 \Rightarrow \text{Beta}(4, 4) \)

- **Employee 2**: 3 clicks out of 10 trials.  
  Likelihood: \( L(\pi) \propto \pi^3 (1 - \pi)^7 \)  
  Posterior: \( f(\pi | y) \propto \pi^6 (1 - \pi)^9 \Rightarrow \text{Beta}(7, 10) \)

- **Employee 3**: 20 clicks out of 100 trials.  
  Likelihood: \( L(\pi) \propto \pi^{20} (1 - \pi)^{80} \)  
  Posterior: \( f(\pi | y) \propto \pi^{23} (1 - \pi)^{82} \Rightarrow \text{Beta}(24, 83) \)

### c) Plot Prior, Likelihood, Posterior for Each Employee
```{r}
library(ggplot2)

df <- data.frame(
  pi = seq(0, 1, length.out = 1000)
) %>%
  mutate(
    prior = dbeta(pi, 4, 3),
    like1 = dbinom(0, 1, pi),
    post1 = dbeta(pi, 4, 4),
    like2 = dbinom(3, 10, pi),
    post2 = dbeta(pi, 7, 10),
    like3 = dbinom(20, 100, pi),
    post3 = dbeta(pi, 24, 83)
  )

ggplot(df, aes(x = pi)) +
  geom_line(aes(y = prior), color = "black", linetype = "dashed") +
  geom_line(aes(y = like1), color = "red") +
  geom_line(aes(y = post1), color = "blue") +
  labs(title = "Employee 1: Prior, Likelihood, Posterior")

ggplot(df, aes(x = pi)) +
  geom_line(aes(y = prior), color = "black", linetype = "dashed") +
  geom_line(aes(y = like2), color = "red") +
  geom_line(aes(y = post2), color = "blue") +
  labs(title = "Employee 2: Prior, Likelihood, Posterior")

ggplot(df, aes(x = pi)) +
  geom_line(aes(y = prior), color = "black", linetype = "dashed") +
  geom_line(aes(y = like3), color = "red") +
  geom_line(aes(y = post3), color = "blue") +
  labs(title = "Employee 3: Prior, Likelihood, Posterior")
```

### d) Summary and Comparison
- All three employees share the same prior (Beta(4,3)).
- Employee 1 has very limited data (1 trial), so the posterior (Beta(4,4)) remains close to the prior.
- Employee 2 has more data; their posterior shifts toward \( \pi = 0.3 \).
- Employee 3, with much more data, shows the most dramatic update, with a posterior centered near \( \pi = 0.2 \).

**Conclusion:** With increasing data, the influence of the prior diminishes, and the posterior is driven more by the data.

## Exercise 4.18: A Sequential Employee

### a) Posterior updates day-by-day

- **Day 1**: Uses Employee 1’s data → Posterior: Beta(4, 4)
- **Day 2**: Updates with Employee 2’s data (3 successes in 10 trials) → Posterior: Beta(7, 14)
- **Day 3**: Updates with Employee 3’s data (20 successes in 100 trials) → Posterior: Beta(27, 97)

### b) Plot each posterior stage

```{r}
plot_beta(4, 4) + ggtitle("Day 1: Posterior")
plot_beta(7, 14) + ggtitle("Day 2: Posterior")
plot_beta(27, 97) + ggtitle("Day 3: Posterior")

```

**Interpretation**: The posterior sharpens and shifts left as more evidence accumulates that the click rate is lower than expected.

### c) What if updated all at once?

If the employee waited until day 3 and pooled all the data:

- Total: 0 + 3 + 20 = 23 successes, 1 + 10 + 100 = 111 trials
- Posterior = Beta(4 + 23, 3 + 88) = Beta(27, 91)

This is slightly different from sequential updating, due to order of computation but **numerically very close** to the sequential final posterior (Beta(27,97)), showing **Bayesian consistency across methods**.
