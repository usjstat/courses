---
title: "Chapter 1: Exploratory Data Analysis - Practical Statistics for Data Science"
author: Simplified by Dr. Rajitha M. Silva
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This chapter introduces the concept of Exploratory Data Analysis (EDA), a foundational practice in Data Science for understanding data before any formal modeling. EDA was popularized by John Tukey in 1962 and emphasizes visualization and summary statistics.

## Elements of Structured Data

Data can be structured or unstructured. To analyze using statistical tools, we often convert unstructured data into a structured, tabular format (like spreadsheets or data frames).

There are two major data types:

- **Numeric**: Continuous (e.g., height), Discrete (e.g., number of children)
- **Categorical**: Ordinal (e.g., education level), Nominal (e.g., gender), Binary (e.g., yes/no)

```{r}
# Example of structured cricket data in R
cricket_data <- data.frame(
  Player = c("Kohli", "Root", "Smith", "Babar"),
  Runs = c(82, 45, 109, 67),
  StrikeRate = c(92.5, 78.4, 101.2, 88.6),
  IsCaptain = c(TRUE, FALSE, TRUE, FALSE)
)
str(cricket_data)
```

## Rectangular Data

Rectangular data structures (tables) are standard for analysis, with:

- **Rows** = records/observations
- **Columns** = variables/features

```{r}
# Example of a data frame with cricket match summary
df <- data.frame(
  Match = c("Ind vs Aus", "Eng vs NZ", "Pak vs SA", "SL vs WI"),
  TotalRuns = c(302, 280, 290, 310),
  WicketsLost = c(6, 8, 7, 5),
  Result = c("Win", "Loss", "Win", "Win")
)
head(df)
```

## Data Types Matter

R handles categorical variables as `factor`s, which are useful for modeling and plotting.

```{r}
# Demonstrating factor vs character with match outcomes
match_result <- c("Win", "Loss", "Draw")
factor_result <- factor(match_result, ordered = TRUE, levels = c("Loss", "Draw", "Win"))
factor_result
```

## Data Frames and Indexes

In R, a `data.frame` uses row order as an implicit index. Other packages such as `data.table` or `dplyr` can enhance indexing and provide faster operations.

```{r}
library(dplyr)
# Indexing with cricket team data
team_data <- tibble(
  TeamID = 1:3,
  TeamName = c("India", "Australia", "Pakistan")
)
team_data
```

In Python, pandas DataFrames are the equivalent structure. They also use row indexes by default and support user-defined and hierarchical indexes:

```python
import pandas as pd

data_tbl = pd.DataFrame({'ID': [1, 2, 3], 'Name': ['A', 'B', 'C']})
print(data_tbl)
```

## Terminology Differences Between R, Python, and Statistics

There are terminology differences depending on the background:

| Concept         | Statistics         | Data Science / Python | R                |
|----------------|--------------------|------------------------|------------------|
| Row            | Observation, Case  | Sample, Record         | Observation       |
| Column         | Variable           | Feature, Attribute     | Variable          |
| Dependent Var  | Response           | Target                 | Response          |
| Independent Var| Predictor          | Feature                | Predictor         |

This confusion can lead to misunderstandings. For example, a "sample" in statistics means a set of observations from a population, while in data science, a "sample" may refer to a single observation (row).

## Non-Rectangular Data Structures

Not all data is rectangular. Other forms include:

- **Time Series**: measurements over time
- **Spatial Data**: data with geographic components
- **Graphs/Networks**: nodes and edges (e.g., social networks)

```{r}
# Simulate a time series of runs scored per over
set.seed(123)
runs_per_over <- ts(sample(0:20, 12, replace = TRUE), start = c(2023, 1), frequency = 1)
runs_per_over
```

Python (using pandas):

```python
import numpy as np
import pandas as pd

np.random.seed(123)
ts_data = pd.Series(np.random.randn(12), index=pd.date_range("2023-01-01", periods=12, freq='M'))
print(ts_data)
```

These formats require special tools and packages (e.g., `xts`, `sf`, or `igraph` in R; `pandas`, `geopandas`, or `networkx` in Python). Still, rectangular data remains the most common format in data science.

## Estimates of Location

To understand where most data values lie:

- **Mean**: Average
- **Median**: Middle value
- **Trimmed Mean**: Removes extremes before averaging
- **Weighted Mean**: Mean adjusted by weights

```{r}
# Runs scored by a player in 4 matches
runs <- c(45, 55, 35, 60)
mean(runs) # Regular mean
mean(runs, trim = 0.25) # Trimmed mean
median(runs) # Median
```

```{r}
# Weighted mean of runs based on balls faced
runs <- c(45, 60, 70)
balls_faced <- c(30, 50, 60)
weighted.mean(runs, w = balls_faced)
```

In Python:

```python
import numpy as np
x = np.array([5, 10, 15])
wts = np.array([1, 2, 1])
np.average(x, weights=wts)
```

## Robust Estimates

Robust statistics are less sensitive to outliers:

```{r}
# Runs in 4 matches with an outlier performance
runs_outlier <- c(10, 15, 18, 150)
mean(runs_outlier) # Affected by outlier
median(runs_outlier) # Robust to outlier
```

```{r}
library(matrixStats)
weightedMedian(x = c(5, 10, 15), w = c(1, 2, 1))
```

In Python:

```python
import wquantiles
wquantiles.median(x, weights=wts)
```

## Estimates of Variability

Variability measures how spread out the values are in a dataset. Key measures include:

- **Variance**: average of squared deviations from the mean
- **Standard Deviation**: square root of variance
- **Mean Absolute Deviation (MAD)**: mean of absolute deviations from the mean
- **Median Absolute Deviation**: median of absolute deviations from the median (robust)
- **Interquartile Range (IQR)**: difference between 75th and 25th percentiles

```{r}
# Bowling economy rates across matches
economy <- c(3.8, 4.2, 3.5, 4.0, 3.7)

# Variance and standard deviation
var(economy)
sd(economy)

# Mean absolute deviation
mean(abs(economy - mean(economy)))

# Median absolute deviation
mad(economy)

# Interquartile range
IQR(economy)
```

In Python:

```python
import numpy as np
import pandas as pd
from statsmodels import robust

x = np.array([1, 4, 4])

np.var(x, ddof=1)  # sample variance
np.std(x, ddof=1)  # sample standard deviation
np.mean(np.abs(x - np.mean(x)))  # MAD
robust.mad(x)  # Median absolute deviation

q75, q25 = np.percentile(x, [75, 25])
iqr = q75 - q25
print(iqr)
```

Each metric gives us different insights. Standard deviation is most commonly reported, but it is not robust. For outlier-prone data, MAD or IQR is often preferred.

## Degrees of Freedom: Why n – 1?

When calculating variance from a **sample**, we divide by `n - 1` instead of `n` to avoid **underestimating** the population variance. This adjustment is known as using the **degrees of freedom**.

- Dividing by `n` results in a **biased** (too small) estimate of the population variance.
- Dividing by `n - 1` gives an **unbiased** estimate.

```{r}
# Variance in delivery speeds (in km/h)
delivery_speeds <- c(135, 138, 142, 145)

var_n <- function(x) {
  mean((x - mean(x))^2)
}

# Biased and unbiased variance
var_n(delivery_speeds)          # Biased
var(delivery_speeds)            # Unbiased (default in R)
```

This difference is usually minor for large datasets but can be important in small samples. The reason for dividing by `n - 1` has to do with how much freedom the data has to vary when we already used some of that information (e.g., when we first calculated the mean). Since we use the sample mean in the variance formula, we lose 1 degree of freedom. That’s why we divide by `n - 1` instead of `n`. Fortunately, in practice, most data science problems don’t require worrying deeply about this unless you're working with very small datasets.

---

## Estimates Based on Percentiles

Percentiles are useful for understanding the distribution of player performance. A **percentile** is a value below which a given percentage of observations fall.

- The **25th percentile** is the value below which 25% of the data lie.
- The **50th percentile** is the **median**.
- The **75th percentile** shows the value below which 75% of the data lie.
- The **interquartile range (IQR)** is the range between the 75th and 25th percentiles.

Percentiles are especially helpful in cricket when analyzing batting scores or bowling figures with potential outliers.

```{r}
# Using quantile to compute percentiles of runs scored
runs <- c(15, 30, 45, 60, 75, 90, 105, 120)
quantile(runs, probs = c(0.25, 0.5, 0.75))  # 25th, 50th, 75th percentiles
IQR(runs)  # Interquartile range
```

In Python:

```python
import numpy as np
runs = np.array([15, 30, 45, 60, 75, 90, 105, 120])
np.percentile(runs, [25, 50, 75])
np.percentile(runs, 75) - np.percentile(runs, 25)  # IQR
```

**Note:** R provides 9 different methods to calculate quantiles (`type` argument in `quantile()`), while Python typically uses linear interpolation.

Percentiles and IQR are **robust**—less sensitive to outliers than standard deviation.

---

## Exploring the Data Distribution

To better understand data distributions in cricket, we can use visual tools such as **histograms**, **density plots**, and **boxplots**. These plots help us detect the shape of the data, presence of skewness, multiple peaks (modes), and outliers.

Let’s explore the distribution of runs scored by a batsman over 10 innings:

```{r}
# Sample batting scores across 10 innings
scores <- c(15, 22, 34, 45, 55, 65, 70, 90, 120, 130)

# Histogram to see frequency distribution
hist(scores,
     main = "Histogram of Batting Scores",
     xlab = "Runs",
     col = "skyblue",
     border = "white")

# Density plot to view smoothed distribution
plot(density(scores),
     main = "Density Plot of Batting Scores",
     xlab = "Runs",
     col = "blue",
     lwd = 2)

# Boxplot to highlight median, IQR, and outliers
boxplot(scores,
        main = "Boxplot of Batting Scores",
        ylab = "Runs",
        col = "orange",
        horizontal = TRUE)
```

### Key Terms
- **Histogram**: Displays how frequently each range of values appears.
- **Density Plot**: A smooth curve that estimates the distribution of the data.
- **Boxplot**: Highlights the **median**, **interquartile range (IQR)**, and **potential outliers**. Useful for comparing across players or formats.

These visualizations are essential for understanding player performance, identifying outliers (like a century among low scores), or checking consistency (tight IQR).

---

## Exploring Binary and Categorical Data

Cricket datasets often include **binary** variables (e.g., whether a batsman is a captain or not) and **categorical** variables (e.g., player role, team name, match outcome).

For binary and categorical data, frequency tables and bar plots are commonly used.

```{r}
# Binary variable: Captaincy status of players
captaincy <- c("Yes", "No", "Yes", "No", "Yes", "No", "No")
table(captaincy)
barplot(table(captaincy),
        main = "Captaincy Distribution",
        ylab = "Count",
        col = c("steelblue", "salmon"))

# Categorical variable: Player roles
roles <- c("Batsman", "Bowler", "All-Rounder", "Batsman", "Bowler", "Bowler")
table(roles)
barplot(table(roles),
        main = "Distribution of Player Roles",
        ylab = "Number of Players",
        col = rainbow(3))
```
```{r}
# Mode: Most frequent player role
roles <- c("Batsman", "Bowler", "All-Rounder", "Batsman", "Bowler", "Bowler")
mode_role <- names(sort(table(roles), decreasing = TRUE))[1]
mode_role
```

```{r}
# Pie chart for match outcomes
match_outcomes <- c("Win", "Win", "Loss", "Win", "Loss", "Loss", "Win")
outcome_counts <- table(match_outcomes)
pie(outcome_counts, main = "Match Outcomes", col = c("green", "red"))
```

```
match_outcomes <- c("Win", "Win", "Loss", "Win", "Loss", "Loss", "Win")
outcome_counts <- table(match_outcomes)
pie(outcome_counts, main = "Match Outcomes", col = c("green", "red"))
```

### Expected value

If we associate categories with values, we can compute the expected value.

```{r}
# Expected value: Win = 1, Loss = 0
match_outcomes <- c("Win", "Win", "Loss", "Win", "Loss", "Loss", "Win")
win_numeric <- ifelse(match_outcomes == "Win", 1, 0)
expected_win <- mean(win_numeric)
expected_win
```

## Probability

Probability gives a way to quantify uncertainty. For example, we can estimate the probability of winning a match based on past results.

```{r}
# Empirical probability of match outcome
outcomes <- table(match_outcomes)
probabilities <- prop.table(outcomes)
probabilities
```

- The result tells us the estimated chance of a win or loss based on historical match data.

## Correlation

Correlation tells us how strongly two numeric variables move together. In cricket, we can examine whether players who score more also tend to have a higher strike rate.

```{r}
# Runs and strike rate for 6 players
runs <- c(45, 70, 65, 30, 90, 100)
strike_rate <- c(78.5, 95.2, 88.4, 70.1, 102.3, 110.4)

# Correlation coefficient
cor(runs, strike_rate)

# Scatterplot to visualize the relationship
plot(runs, strike_rate,
     main = "Correlation between Runs and Strike Rate",
     xlab = "Runs",
     ylab = "Strike Rate",
     pch = 19,
     col = "darkgreen")
abline(lm(strike_rate ~ runs), col = "red")
```

A positive correlation indicates that as one variable increases (e.g., runs), the other (e.g., strike rate) also tends to increase.

We can also visualize correlations between multiple numeric features using a **heatmap**.

```{r}
library(ggplot2)
library(reshape2)

# Updated cricket stats with more variability
cricket_stats <- data.frame(
  Runs = c(45, 70, 65, 30, 90, 100),
  StrikeRate = c(78.5, 95.2, 88.4, 70.1, 102.3, 110.4),
  BallsFaced = c(65, 48, 60, 50, 66, 72),
  Fours = c(5, 6, 5, 4, 7, 3),
  Sixes = c(2, 3, 1, 0, 4, 5)
)

# Compute correlation matrix
cor_matrix <- round(cor(cricket_stats), 2)

# Melt and plot heatmap
melted_cor <- melt(cor_matrix)
ggplot(data = melted_cor, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", 
                       name="Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                   size = 10, hjust = 1)) +
  labs(title = "Correlation Heatmap: Player Stats", x = "", y = "")
```

This heatmap shows how different player performance metrics are related to each other. For instance:
- **Runs** and **StrikeRate** have a strong positive correlation, meaning players who score more tend to have higher strike rates.
- **Runs** and **BallsFaced** also show a positive correlation—longer innings usually lead to higher scores.
- **Sixes** may not correlate strongly with **BallsFaced**, which can indicate aggressive hitting in shorter time.

Blue colors represent negative or weak correlations, while red colors represent strong positive ones. This helps analysts quickly spot which stats tend to move together. For example, strong positive correlation between Runs and StrikeRate or BallsFaced can be expected, while weaker or negative correlations may indicate independent or inverse relationships. 

## Exploring Two or More Variables

While estimators like mean and variance describe single variables (**univariate analysis**), exploring relationships between two or more variables offers deeper insight.

- **Bivariate analysis** compares two variables (e.g., runs vs. strike rate).
- **Multivariate analysis** explores interactions among three or more variables.

### Key Terms for Exploring Two or More Variables
- **Contingency table**: A tally of counts between two or more categorical variables.
- **Hexagonal binning**: A scatterplot with numeric variables grouped into hexagonal bins.
- **Contour plot**: A density-based topographical view of two numeric variables.
- **Violin plot**: Combines boxplot with a density plot to show distribution shape.

```{r}
# Contingency table: Player position vs match result
position <- c("Forward", "Midfielder", "Forward", "Defender", "Midfielder", "Defender", "Forward")
result <- c("Win", "Loss", "Win", "Win", "Loss", "Loss", "Win")
table(position, result)
```

```python
import pandas as pd
position = ["Forward", "Midfielder", "Forward", "Defender", "Midfielder", "Defender", "Forward"]
result = ["Win", "Loss", "Win", "Win", "Loss", "Loss", "Win"]
df = pd.DataFrame({'Position': position, 'Result': result})
print(pd.crosstab(df['Position'], df['Result']))
```
```{r}
library(hexbin)
library(ggplot2)
library(dplyr)

# Simulated cricket batting stats: Runs and Balls Faced in 50 matches
set.seed(123)
runs <- rpois(50, lambda = 40)
balls_faced <- rpois(50, lambda = 35)

# Create data frame
batting <- data.frame(Runs = runs, BallsFaced = balls_faced)

# More illustrative hexbin plot
ggplot(batting, aes(x = BallsFaced, y = Runs)) +
  stat_binhex(bins = 10) +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(title = "Hexbin Plot: Runs vs Balls Faced",
       x = "Balls Faced",
       y = "Runs",
       fill = "Count") +
  theme_minimal()
```

he hexbin plot above visualizes the relationship between **Runs** and **Balls Faced** across 50 matches. Each hexagon represents a bin, and its color indicates the number of observations that fall within that bin.

- **Darker blue hexagons** indicate more frequent combinations of Runs and Balls Faced.
- You’ll notice that most observations fall around the 35–45 ball range and 30–50 run range.
- This suggests players commonly score in that zone, and longer innings (more balls faced) often lead to more runs.


```python
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

speed = np.array([24, 22, 23, 25, 20, 21, 22, 23, 24, 25])
endurance = np.array([88, 90, 85, 92, 80, 84, 86, 87, 90, 91])
plt.hexbin(speed, endurance, gridsize=10, cmap='Greens')
plt.xlabel("Speed (km/h)")
plt.ylabel("Endurance Score")
plt.title("Hexbin Plot: Speed vs Endurance")
plt.colorbar()
plt.show()
```


```{r}
# Contour plot: Batting average vs Strike Rate
library(MASS)
avg <- c(45, 50, 52, 48, 60, 62, 40, 55, 47, 53)
sr <- c(78, 82, 85, 80, 95, 100, 70, 90, 76, 88)
dens <- kde2d(avg, sr, n = 50)
contour(dens, xlab = "Batting Average", ylab = "Strike Rate",
        main = "Contour Plot: Average vs Strike Rate")
```

The contour plot visualizes the **joint density** of two continuous variables—in this case, **Batting Average** and **Strike Rate**.

- Lines on the plot represent areas of equal density (like elevation contours on a topographic map).
- **Tightly packed lines** indicate areas with higher concentration of observations.
- The plot suggests that most players have a batting average between 45 and 55 and a strike rate between 75 and 95.

Contour plots are especially useful for detecting patterns or clusters in two-dimensional numeric data and for identifying regions of high density.


```python
from scipy.stats import gaussian_kde
import numpy as np
import matplotlib.pyplot as plt

avg = np.array([45, 50, 52, 48, 60, 62, 40, 55, 47, 53])
sr = np.array([78, 82, 85, 80, 95, 100, 70, 90, 76, 88])
k = gaussian_kde([avg, sr])
x, y = np.mgrid[40:65:100j, 70:105:100j]
pos = np.vstack([x.ravel(), y.ravel()])
z = np.reshape(k(pos).T, x.shape)
plt.contour(x, y, z)
plt.xlabel("Batting Average")
plt.ylabel("Strike Rate")
plt.title("Contour Plot: Average vs Strike Rate")
plt.show()
```

```{r}
# Violin plot: 100m sprint times by gender
library(ggplot2)
sprint_data <- data.frame(
  Gender = rep(c("Male", "Female"), each = 15),
  Time = c(rnorm(15, 10.5, 0.3), rnorm(15, 12.0, 0.4))
)

ggplot(sprint_data, aes(x = Gender, y = Time, fill = Gender)) +
  geom_violin(trim = FALSE) +
  geom_boxplot(width = 0.1, fill = "white") +
  labs(title = "Violin Plot: 100m Sprint Times by Gender") +
  theme_minimal()
```

```python
import seaborn as sns
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

genders = ["Male"] * 15 + ["Female"] * 15
times = np.concatenate([np.random.normal(10.5, 0.3, 15), np.random.normal(12.0, 0.4, 15)])
df = pd.DataFrame({'Gender': genders, 'Time': times})
sns.violinplot(x="Gender", y="Time", data=df, inner="box")
plt.title("Violin Plot: 100m Sprint Times by Gender")
plt.show()
```

These tools help uncover patterns and interactions among variables, like performance differences by player position, role, gender, or physical metrics.


 
