---
title: "Probability and Distribution Theory: Lesson 3"
author: "Dr. Rajitha M. Silva"
output: html_notebook
runtime: shiny
---

# 1.3 Axioms of Probability

Axioms form the foundation of probability theory. These are the basic rules or assumptions upon which all of probability is built. They allow us to develop a consistent mathematical framework for assigning and manipulating probabilities.

Understanding these axioms is important because:\

-   They ensure that the probability of any event is a meaningful, non-negative number.\
-   They guarantee the total probability across all possible outcomes equals 1.\
-   They help us handle more complex situations involving multiple events.

Probability is a numerical measure that quantifies the likelihood of an event occurring.

A probability function $\text{Pr}(\cdot)$ must satisfy these **three axioms**:

-   **Axiom 1**: For any event $A$, $\text{Pr}(A) \geq 0$
-   **Axiom 2**: $\text{Pr}(S) = 1$ (i.e., the total probability of the sample space is 1)
-   **Axiom 3**: For any countable collection of **mutually exclusive** events $A_1, A_2, \ldots$, $$ \text{Pr}(A_1 \cup A_2 \cup \ldots) = \sum_i \text{Pr}(A_i) $$

This holds for both **finite** and **countable infinite** unions.

------------------------------------------------------------------------

# 1.4 Methods for Determining Probability

Probability can be determined in different ways depending on the context. Whether based on logical reasoning, past data, or subjective belief, each method has its own use case and importance.

Understanding these methods helps you:

\- Decide which model or approach fits a given situation.

\- Interpret probability values meaningfully.

\- Apply theory to real-world settings, such as games, experiments, or business problems.

There are four methods used to determine probabilities:

1.  **Classical Method**: Based on equally likely outcomes in a finite sample space.
2.  **Relative Frequency Method**: Based on repeated experiments and observed frequencies.
3.  **Subjective Method**: Based on personal judgment or opinion.
4.  **Using Probability Models**: Based on theoretical assumptions.

------------------------------------------------------------------------

## 1.4.1 Classical Method

The classical method of probability is the most basic and widely taught approach. It is grounded in situations where all outcomes are assumed to be **equally likely** - that is, each outcome has the same chance of occurring.

### What Does "Equally Likely" Mean?

Equally likely outcomes mean that, based on the design of the experiment or inherent symmetry, there is no reason to believe one outcome will happen more often than another. For example:

-   Each face of a fair die has a 1/6 chance.\
-   Each side of a fair coin has a 1/2 chance.\
-   Each card in a well-shuffled deck has a 1/52 chance.\

This method is especially useful when dealing with finite, symmetric, and well-defined sample spaces, such as dice rolls, card draws, or coin tosses.

If all outcomes in the sample space $S$ are equally likely, then for any event $E$:

$$
\text{Pr}(E) = \frac{n(E)}{n(S)}
$$

------------------------------------------------------------------------

## Activity 1.8

A fair six-sided die is rolled. Find the probabilities of the following events:

1.  $A$: getting the number 1\
2.  $B$: getting a number less than 3\
3.  $C$: getting an even number\
4.  $D$: getting a number greater than 5\
5.  $E$: getting a number greater than 6

------------------------------------------------------------------------



# **Counting Techniques**

Counting techniques are essential tools in probability theory for calculating the number of possible outcomes in complex experiments. These techniques help us avoid the impractical task of listing every possible outcome when the number of outcomes grows large.

### 1. Multiplication Rule

If one task can be done in $m$ ways and another in $n$ ways, then the two tasks together can be performed in $m \times n$ ways.

**Example:** How many 3-digit PINs can be formed if each digit can be from 0 to 9? $$
10 \times 10 \times 10 = 1000 \text{ PINs}
$$

### 2. Permutations (Ordered Arrangements)

Permutations refer to the number of ways to arrange $r$ items out of $n$, when the order matters.

-   Formula (no repetition): $P(n, r) = \frac{n!}{(n - r)!}$

**Example:** How many ways can 3 medals (gold, silver, bronze) be awarded to 5 athletes? $$
P(n, r) = \frac{n!}{(n - r)!}
$$

$$
P(5, 3) = \frac{5!}{(5 - 3)!} = \frac{120}{2} = 60
$$

### 3. Combinations (Unordered Selections)

Combinations count how many ways $r$ elements can be selected from $n$, where the order doesn't matter.

-   Formula: $$
    C(n, r) = \binom{n}{r} = \frac{n!}{r!(n - r)!}
    $$

$$
\binom{10}{4} = \frac{10!}{4!6!} = 210
$$

**Example:** From 10 applicants, how many ways can a committee of 4 be formed? $$
\binom{10}{4} = 210
$$

These techniques help us for solving problems in probability, specially when dealing with complex events such as card games, lotteries, and arrangements.

------------------------------------------------------------------------

## Activity 1.9

A box contains 3 white balls and 2 black balls. Three balls are taken randomly from this box.\
If possible, find the probabilities of the following events using the classical method.\
Assume the 3 balls are drawn simultaneously (i.e., order does not matter).

1.  $A$: the event of getting exactly one white ball\
2.  $B$: the event of getting more than one white ball\
3.  $C$: the event of getting no white balls


## 1.4.2 Relative Frequency Method

The **relative frequency method** is another way to estimate probability, specially useful when the theoretical model is difficult to define, or when real-world data is available from repeated observations.

### Key Ideas

-   Suppose we repeat an experiment many times under **identical and independent conditions**.
-   Let $E$ be an event we are interested in observing (e.g., getting a six when rolling a die).
-   Let $\text{freq}(E)$ be the number of times the event $E$ occurs in $n$ trials.

Then the **relative frequency** of event $E$ is:

$$
\text{Relative Frequency} = \frac{\text{freq}(E)}{n}
$$

As $n$ becomes large, the relative frequency tends to **stabilize**, approaching the **true probability** of the event. This leads us to the idea:

$$
\text{Pr}(E) = \lim_{n \to \infty} \frac{\text{freq}(E)}{n}
$$

This interpretation of probability is known as the **relative frequency definition**.

> **When to use:**\
> This method is especially helpful when you **collect data** from a real or simulated experiment and want to estimate the probability based on outcomes observed over many repetitions.

### Simulating an Example in R

We now explore how the relative frequency converges to the true probability as the number of trials increases.

```{r echo=FALSE}
library(shiny)

shinyApp(
  ui = fluidPage(
    titlePanel("Relative Frequency of Rolling a 6"),
    sidebarLayout(
      sidebarPanel(
        sliderInput("n", "Number of Trials:", 
                    min = 100, max = 10000, value = 1000, step = 100)
      ),
      mainPanel(
        plotOutput("freqPlot"),
        verbatimTextOutput("finalFreq")
      )
    )
  ),
  
  server = function(input, output) {
    output$freqPlot <- renderPlot({
      rolls <- sample(1:6, size = input$n, replace = TRUE)
      rel_freq <- cumsum(rolls == 6) / seq_along(rolls)
      
      plot(rel_freq, type = "l", col = "blue", lwd = 2,
           ylim = c(0, 0.4),
           xlab = "Number of Trials", 
           ylab = "Relative Frequency of Rolling a 6",
           main = "Convergence of Relative Frequency to Theoretical Probability")
      abline(h = 1/6, col = "red", lty = 2)
      legend("topright", legend = c("Relative Frequency", "True Probability = 1/6"),
             col = c("blue", "red"), lty = c(1, 2), bty = "n")
    })
    
    output$finalFreq <- renderPrint({
      rolls <- sample(1:6, size = input$n, replace = TRUE)
      rel_freq <- mean(rolls == 6)
      cat("Relative frequency after", input$n, "trials:", round(rel_freq, 4))
    })
  }
)
```

This estimates $\text{Pr}(6)$ as the proportion of sixes observed in 10,000 simulated rolls. If the die is fair, this value should be close to $1/6 \approx 0.1667$.

## Activity 1.10

In a certain production process, milk powder is filled into bags. A bag is said to be **non-conforming** if its weight is **less than 398g or greater than 403g**. Otherwise, it is said to be **conforming**.

The quality inspector has measured the weights of 585 randomly selected bags. The frequency table of those weights (in grams) is given below:

| Weight (in grams) | Number of Bags |
|:-----------------:|:--------------:|
|      396-397      |       10       |
|      398-399      |      175       |
|      400-401      |      270       |
|      402-403      |      115       |
|      404-405      |       15       |

1.  If a bag is taken randomly from the process, what is the probability that it will be a **non-conforming** bag?

2.  If three bags are taken randomly from the process, what is the probability that **one of them will be non-conforming**?

------------------------------------------------------------------------

## Activity 1.11

In a large manufacturing facility, an opinion survey was conducted regarding three types of bonus schemes. A random sample of 960 employees was selected. The job category and the bonus scheme of each employee were recorded. The results are shown below:

| Employee Category | Type I | Type II | Type III | Total |
|:-----------------:|:------:|:-------:|:--------:|:-----:|
|      Laborer      |  190   |   243   |   197    |  630  |
|       Clerk       |   82   |   44    |    44    |  170  |
|    Technician     |   23   |   78    |    34    |  135  |
|     Executive     |   5    |   12    |    8     |  25   |
|     **Total**     |  300   |   377   |   283    |  960  |

If an employee is selected randomly from this manufacturing facility, what is the probability that:

1.  The employee is a **clerk**?
2.  The employee has **bonus scheme II**?
3.  The employee is a **technician with bonus scheme I**?

------------------------------------------------------------------------

## 1.4.3 Subjective Probability Method

Not all situations allow us to calculate probabilities using past data (relative frequency) or symmetry (classical method). In such cases, we rely on **subjective probability**.

Subjective probability is based on a person's **judgment, intuition, or personal belief** about how likely an event is to occur.

### Key Features

-   A **subjective probability** reflects an individual's **opinion** about the occurrence of an event.
-   These probabilities are **personal**, they can **differ from person to person**, even when considering the same event.
-   It is often used when no past data or mathematical model is available.

### Limitations

-   Assigning a subjective probability can be difficult and may **lack consistency**.
-   There is **no objective way to verify** whether a subjective probability is correct.
-   The value can change with time, new information, or even mood.

### Common Approaches to Estimate Subjective Probabilities

-   **Personal guess** based on experience or intuition\
-   **Betting approach**: How much a person is willing to bet on an event indicates their belief in its occurrence\
-   **Reference lottery approach**: Comparing the uncertainty of the event to a known lottery

**Note**: While subjective probabilities are important in areas like decision making and risk analysis, this method will **not be discussed further in this course.**

The **subjective interpretation of probability** is a foundational idea in **Bayesian Statistics**, where probabilities are used to express **degrees of belief** rather than long-run frequencies.

Bayesian methods allow us to update our beliefs in light of new data using **Bayes' Theorem**. 

------------------------------------------------------------------------

## 1.4.4 Using Probability Models

In some situations, we do not rely on personal opinions (subjective method), experimental results (relative frequency), or symmetry (classical method). Instead, we use **mathematical models** to represent the behavior of random phenomena. These are called **probability models**.

A probability model defines:

-   A **sample space**, all possible outcomes of the experiment
-   A **probability function**,  a rule that assigns probabilities to events in the sample space

This approach is common in scientific, engineering, and data-driven applications where the underlying process can be **described using theoretical assumptions**.

------------------------------------------------------------------------

### Key Features

-   Probability models are often **idealized representations** of real-world processes.
-   They allow us to **make predictions**, calculate **expected outcomes**, and assess **risks**.
-   These models are usually built using **probability distributions** such as:
    -   The binomial distribution (e.g., number of successes in a series of trials)
    -   The normal distribution (e.g., measurement errors, heights)
    -   The Poisson distribution (e.g., number of arrivals per minute)

------------------------------------------------------------------------

### Example

Suppose we are modeling the number of defective items in a batch of 100 products. We might assume that each item has a small, independent chance of being defective and model the number of defectives using the **binomial distribution**.

Or, we might model the amount of rainfall in a day using a **continuous distribution** like the normal or exponential distribution.

> These kinds of probability models are specially useful in designing experiments, simulating systems, and making data-based decisions.

------------------------------------------------------------------------

