---
title: " Statistical Experiments and Significance Testing (ctd)"
author: "Dr. Rajitha M. Silva"
output: html_document
---


## Multiple Testing

### Introduction
Statistical testing is often applied to a single hypothesis. For example:
- Testing whether a new cricket training program improves batting averages.
- Checking if a new shoe brand improves marathon performance.
- Evaluating if a basketball player's free throw success rate is better this season.

In these cases, **one comparison** is made, and the Type 1 error rate (e.g., \( \alpha = 0.05 \)) is straightforward to interpret.

However, in many real-world cases, **multiple hypotheses** are tested simultaneously. This can lead to an increased risk of **false positives** (Type 1 errors).

---

### Key Terms

- **Type 1 Error**: Mistakenly concluding that an effect is statistically significant when it is not (a false positive).
- **False Discovery Rate (FDR)**: The expected proportion of false positives among all the significant results.
- **Adjustment of p-values**: Techniques (like Bonferroni or Benjamini-Hochberg) used to account for multiple testing and reduce Type 1 error risk.
- **Overfitting**: Modeling noise in the data as if it were signal. Common when too many models or tests are conducted.
- **Alpha Inflation**: The increase in the overall probability of making a Type 1 error due to multiple comparisons.
- **Fitting the Noise**: Selecting or tuning a model based on random fluctuations in the data rather than real signal. Can lead to poor generalization.

---

### The Problem of Alpha Inflation

When performing many tests, the chance of finding at least one statistically significant result by chance increases.

#### Example:
Suppose we run 20 tests, each at \( \alpha = 0.05 \). The probability of **at least one** Type 1 error is:

\[
1 - (1 - 0.05)^{20} \approx 0.64
\]

This formula comes from the probability of *not* making any Type 1 error across 20 tests:

\[
(1 - 0.05)^{20} = 0.95^{20} \approx 0.3585
\]

So the complement (probability of at least one Type 1 error) is:

\[
1 - 0.3585 = 0.6415
\]

This shows a **64% chance** of making **at least one false discovery** just by chance when running 20 independent tests at a 5% significance level.

---



---
# Adjusting for Multiple Comparisons



Statisticians developed procedures to handle alpha inflation by setting a more stringent criterion for statistical significance compared to single hypothesis tests. These procedures generally involve \"dividing up\" the alpha across the number of tests, resulting in a smaller alpha for each test. Common methods include:

- **Bonferroni Adjustment**: Dividing alpha by the number of tests.

- **Tukey's Honest Significant Difference (HSD)**: Specifically used for multiple group means comparisons. This approach evaluates the maximum difference among group means against a benchmark based on the t-distribution, similar to a resampling method. The analogy to resampling arises because Tukey's HSD compares observed differences in group means against a distribution generated by assuming no real differences (null hypothesis), similar to permutation or bootstrap approaches. The alpha value in Tukey’s HSD is adjusted to control the Type 1 error rate across all pairwise comparisons simultaneously.

#### Calculation of Tukey’s HSD:

Tukey's HSD is calculated as:

\[
HSD = q_{\alpha, k, df_{error}} \times \sqrt{\frac{MS_{error}}{n}}
\]

where:

- \( q_{\alpha, k, df_{error}} \) is the studentized range statistic,
- \( MS_{error} \) is the mean square error from ANOVA,
- \( n \) is the number of observations per group,
- \( k \) is the number of groups.

Here's how to calculate Tukey's HSD explicitly in R and Python:

#### R Example

```{r}
data <- data.frame(
  group = rep(c("A", "B", "C"), each = 10),
  value = c(rnorm(10, mean=5), rnorm(10, mean=6), rnorm(10, mean=7))
)

anova_results <- aov(value ~ group, data=data)
anova_summary <- summary(anova_results)
MSE <- anova_summary[[1]]$`Mean Sq`[2]

df_error <- anova_summary[[1]]$Df[2]
k <- length(unique(data$group))
n <- length(data$value) / k

q_value <- qtukey(0.95, nmeans=k, df=df_error)
HSD <- q_value * sqrt(MSE / n)
HSD
```

#### Python Example

```python
import numpy as np
import scipy.stats as stats

# Example data
np.random.seed(0)
group_A = np.random.normal(5, 1, 10)
group_B = np.random.normal(6, 1, 10)
group_C = np.random.normal(7, 1, 10)

all_data = np.concatenate([group_A, group_B, group_C])
groups = ['A']*10 + ['B']*10 + ['C']*10

# ANOVA
f_val, p_val = stats.f_oneway(group_A, group_B, group_C)

# Calculate Mean Square Error
MSE = np.var(all_data, ddof=3)

# Degrees of freedom and number of groups
df_error = len(all_data) - 3
k = 3
n = len(group_A)

# Studentized range critical value
q_value = stats.tukey_hsd.ppf(0.95, k, df_error)

# Calculate HSD
HSD = q_value * np.sqrt(MSE / n)
HSD
```



These adjustments set more tighten criteria for declaring statistical significance, mitigating the risk of false positives.

# False Discovery Rate (FDR)

The **False Discovery Rate (FDR)** describes the expected proportion of false discoveries (false positives) among all discoveries (tests declared significant).

FDR emerged notably in genomic research, which often involves tens of thousands of simultaneous hypothesis tests. It controls not the probability of making a single false discovery but rather the proportion of discoveries that are expected to be false.

Mathematically, the FDR is expressed as:

\[
FDR = \frac{\text{False Positives}}{\text{Total Positives}} = \frac{FP}{FP + TP}
\]

Where:
- \( FP \) is False Positives.
- \( TP \) is True Positives.

### FDR and Power

- **FDR** procedures are less strict compared to traditional corrections (e.g., Bonferroni), allowing some false positives intentionally.
- By allowing some false positives, FDR procedures increase the **statistical power**, or the ability to detect true effects. This makes FDR especially useful in exploratory analyses.

### Why suitable for Exploratory Analysis?

- In exploratory settings, researchers often test numerous hypotheses without prior specific predictions. 
- A strict criterion (like Bonferroni) would drastically reduce the chance of discovering true effects.
- **FDR balances the risk of false discoveries against missing potentially interesting results**, making it ideal for exploratory analyses involving many tests.

### Practical Example from Data Science:

Suppose you're analyzing 1000 genetic markers associated with a disease. 
- Using strict correction methods might significantly reduce the chance of finding true associations.
- Applying FDR allows you to discover more potential associations, understanding some will inevitably be false. However, this trade-off is beneficial when further validation studies can confirm true discoveries.

### Implementing FDR in R

Here's an example of applying FDR correction to p-values using the `p.adjust` function in R:

```{r}
# Example p-values
p_values <- c(0.01, 0.04, 0.03, 0.002, 0.06, 0.15, 0.001)

# Adjust p-values using FDR
adjusted_p_values <- p.adjust(p_values, method = "fdr")
adjusted_p_values
```

#### Python Example

```python
from statsmodels.stats.multitest import multipletests

# Example p-values
p_values = [0.01, 0.04, 0.03, 0.002, 0.06, 0.15, 0.001]

# Adjust p-values using FDR
adjusted_p_values = multipletests(p_values, method='fdr_bh')[1]
adjusted_p_values
```

This adjustment helps identify discoveries worth further investigation.

### Cricket Example of FDR

Suppose a cricket analyst tests 50 batting-related performance metrics (such as average score, strike rate, boundaries per innings, etc.) across 100 players to see if any metric significantly predicts match outcomes. Using traditional correction methods (like Bonferroni) might make it difficult to detect meaningful metrics due to overly stringent criteria. Instead, the analyst can apply an FDR approach to find interesting patterns, balancing discoveries and tolerating some false positives.

### Practical implementation in R and Python

**R Example:**

```{r}
# Simulated p-values for 50 tests
set.seed(123)
p_values_cricket <- runif(50, min=0, max=0.1)

# Adjust p-values using FDR
adjusted_p_cricket <- p.adjust(p_values_cricket, method = "fdr")

# Display significant results
significant_cricket <- adjusted_p_cricket[adjusted_p_cricket < 0.05]
significant_cricket
```

**Python Example:**

```python
import numpy as np
from statsmodels.stats.multitest import multipletests

# Simulated p-values for 50 tests
np.random.seed(123)
p_values_cricket = np.random.uniform(0, 0.1, 50)

# Adjust p-values using FDR
adjusted_p_cricket = multipletests(p_values_cricket, method='fdr_bh')[1]

# Display significant results
significant_cricket = adjusted_p_cricket[adjusted_p_cricket < 0.05]
print(significant_cricket)
```

Applying FDR in this way allows the cricket analyst to identify potential metrics worth deeper exploration while managing the rate of false discoveries.


# Degrees of Freedom (DoF)

The **Degrees of Freedom (DoF)** represent the number of independent values in a calculation that are free to vary. This concept is crucial in statistical tests, influencing the shape of various probability distributions such as the t-distribution, chi-square, and F-distribution.

Mathematically, degrees of freedom are often calculated as \(n - 1\) where \(n\) is the total number of observations.

### Sports Example (Cricket):

Suppose you want to analyze the average scores of a cricket player over 10 matches. Once you calculate the average based on these 10 matches, you have 9 degrees of freedom. This is because knowing the average and the first 9 scores makes the 10th score predictable, removing its freedom to vary.

#### Example in R:

```{r}
scores <- c(45, 67, 38, 70, 55, 48, 62, 77, 54, 61)
mean_score <- mean(scores)
DoF <- length(scores) - 1
DoF
```

#### Example in Python:

```python
scores = [45, 67, 38, 70, 55, 48, 62, 77, 54, 61]
mean_score = np.mean(scores)
DoF = len(scores) - 1
print("Degrees of Freedom:", DoF)
```



### Is it important for Data Science?

Not always, especially in the context of significance testing. In data science, formal statistical tests are used sparingly, and sample sizes are often large enough that the difference between using \(n\) and \(n - 1\) becomes negligible. As \(n\) increases, the bias introduced by using \(n\) instead of \(n - 1\) in the denominator disappears.

However, degrees of freedom matter in regression modeling. In particular, problems can occur when creating dummy variables for categorical predictors. For example, the variable "day of week" has seven levels but only six degrees of freedom. If dummy variables are created for all seven days, one variable becomes redundant and causes multicollinearity, leading to model failure. This issue is relevant for both linear and logistic regression in data science workflows.

### Dummy Variables 

Dummy variables are commonly used to represent categorical information such as player positions, match venues, or days of the week in sports. For instance, in a cricket dataset, the match location might be encoded as "Home" or "Away," or the batting position could range from 1 to 11. Dummy variables allow such categories to be represented in regression models.

However, when creating dummy variables, it's crucial to avoid multicollinearity caused by including all levels of a categorical variable. For example, consider the variable "Batting Position" with 5 positions. If we create 5 dummy variables (one for each position) and include all of them in a regression model along with an intercept, we introduce perfect multicollinearity—the sum of the dummy variables equals 1 for every observation.

This problem is addressed by using only \(k - 1\) dummy variables, where \(k\) is the number of categories. The omitted category becomes the reference group. For example:

```{r}
# R Example: Dummy coding for batting positions
batting_pos <- factor(c("Opener", "Middle", "Middle", "Lower", "Opener"))
dummies <- model.matrix(~ batting_pos)[, -1]  # drop intercept and first level
print(dummies)
```

```python
# Python Example: Dummy coding for batting positions
import pandas as pd
batting_pos = pd.Series(["Opener", "Middle", "Middle", "Lower", "Opener"])
dummies = pd.get_dummies(batting_pos, drop_first=True)
print(dummies)
```

By dropping one dummy variable, we maintain full rank and avoid multicollinearity. In sports models, this step is vital to ensure reliable coefficient estimates and proper interpretation of effects.

### ANOVA and Permutation Testing

Let's illustrate this with a cricket-based sports example using permutation ANOVA. We test whether average reaction times differ significantly across four different cricket training strategies: Power, Agility, Endurance, and Control.

Suppose we're comparing the effectiveness of four cricket training strategies (A, B, C, D) by measuring players' improvement scores. We collect five improvement scores for each strategy.

We can test the significance of the observed variance among group means using this resampling procedure:

1. Combine all the data into a single pool.
2. Shuffle the values and draw four resamples of five values each (representing A, B, C, and D).
3. Calculate the mean of each group.
4. Record the variance of these group means.
5. Repeat steps 2–4 at least 1,000 times.
6. Determine the proportion of these permutations where the resampled variance exceeds the actual observed variance.

This proportion gives a **p-value** indicating how unusual the observed result is under the null hypothesis of no difference.

In R, this type of permutation-based ANOVA can be carried out using the `aovp` function from the `lmPerm` package:

```{r}
# install.packages("lmPerm")
library(lmPerm)
# Simulated reaction time data for 4 cricket training strategies
set.seed(123)
cricket_sessions <- data.frame(
  TrainingFormat = rep(c("Power", "Agility", "Endurance", "Control"), each = 5),
  ReactionTime = c(
    rnorm(5, mean = 0.45, sd = 0.05),
    rnorm(5, mean = 0.43, sd = 0.04),
    rnorm(5, mean = 0.47, sd = 0.06),
    rnorm(5, mean = 0.44, sd = 0.05)
  )
)

# Permutation ANOVA using lmPerm
result <- aovp(ReactionTime ~ TrainingFormat, data = cricket_sessions, perm = "Exact")
summary(result)
```

This method does not rely on normality assumptions and provides a flexible way to conduct inference, particularly useful when sample sizes are small or assumptions are violated.

```{r}
# Simulated reaction time data for 4 cricket training strategies
set.seed(123)
cricket_sessions <- data.frame(
  TrainingFormat = rep(c("Power", "Agility", "Endurance", "Control"), each = 5),
  ReactionTime = c(
    rnorm(5, mean = 0.45, sd = 0.05),
    rnorm(5, mean = 0.43, sd = 0.04),
    rnorm(5, mean = 0.47, sd = 0.06),
    rnorm(5, mean = 0.44, sd = 0.05)
  )
)

# Permutation ANOVA using lmPerm
result <- aovp(ReactionTime ~ TrainingFormat, data = cricket_sessions, perm = "Exact")
summary(result)
```




Here, the p-value (`Pr(Prob)`) (which stands for "Probability of Observed (or more extreme) value under the null hypothesis") is 0.3209, indicating that under the assumption of equal effectiveness among training formats, about 9.3% of permutations resulted in a variance as extreme as what was observed. Since this p-value is above the conventional 0.05 threshold, we do not reject the null hypothesis; the differences in reaction times could have arisen by chance.

The `Iter` column shows the number of resampling iterations. The `Df`, `R Sum Sq`, and `R Mean Sq` columns mirror a traditional ANOVA table.
```

Here, `Pr(Prob)` in the output is the p-value obtained by comparing the observed variance among group means to the distribution of variances from permuted datasets. A value like 0.3209 means that in about 3.2% of the random permutations, a variance as extreme as the observed one was found—indicating the result is not statistically significant at the 5% level.
```

This method does not rely on normality assumptions and provides a flexible way to conduct inference, particularly useful when sample sizes are small or assumptions are violated.
